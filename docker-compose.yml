name: folio

services:
  comfyui:
    build: ./docker/comfyui-cpu
    ports:
      - "8188:8188"
    volumes:
      - ./models:/opt/ComfyUI/models
      - ./storage/comfyui-output:/opt/ComfyUI/output
    environment:
      - COMFYUI_ARGS=--listen 0.0.0.0 --cpu
      - WEB_ENABLE_AUTH=false
      - SKIP_ACL=true
      - CF_QUICK_TUNNELS=false
      - SERVICEPORTAL_PORT_HOST=
      - SYNCTHING_PORT_HOST=
    profiles:
      - cpu

  comfyui-nvidia:
    build: ./docker/comfyui
    runtime: nvidia
    profiles:
      - gpu
    ports:
      - "8188:8188"
    volumes:
      - ./models:/opt/ComfyUI/models
      - ./storage/comfyui-output:/opt/ComfyUI/output
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - GPU_COUNT=1
      - COMFYUI_ARGS=--listen 0.0.0.0
      - WEB_ENABLE_AUTH=false
      - SKIP_ACL=true
      - CF_QUICK_TUNNELS=false
      - SERVICEPORTAL_PORT_HOST=
      - SYNCTHING_PORT_HOST=
    networks:
      default:
        aliases:
          - comfyui
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  comfyui-rocm:
    build: ./docker/comfyui-rocm
    profiles:
      - rocm
    ports:
      - "8188:8188"
    volumes:
      - ./models:/opt/ComfyUI/models
      - ./storage/comfyui-output:/opt/ComfyUI/output
    environment:
      - GPU_COUNT=1
      - COMFYUI_ARGS=--listen 0.0.0.0
      - WEB_ENABLE_AUTH=false
      - SKIP_ACL=true
      - CF_QUICK_TUNNELS=false
      - SERVICEPORTAL_PORT_HOST=
      - SYNCTHING_PORT_HOST=
    networks:
      default:
        aliases:
          - comfyui
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render

  # SGLang LLM Server - CPU profile (slow, for testing only)
  sglang:
    build: ./docker/sglang-cpu
    ports:
      - "30000:30000"
    volumes:
      - ./storage/huggingface:/root/.cache/huggingface
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
    profiles:
      - cpu
    command: ["python", "-m", "sglang.launch_server", "--host", "0.0.0.0", "--port", "30000", "--model-path", "${SGLANG_MODEL:-meta-llama/Llama-3.2-1B-Instruct}"]

  # SGLang LLM Server - NVIDIA GPU profile
  sglang-nvidia:
    build: ./docker/sglang
    runtime: nvidia
    profiles:
      - gpu
    ports:
      - "30000:30000"
    volumes:
      - ./storage/huggingface:/root/.cache/huggingface
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN:-}
    networks:
      default:
        aliases:
          - sglang
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: ["python", "-m", "sglang.launch_server", "--host", "0.0.0.0", "--port", "30000", "--model-path", "${SGLANG_MODEL:-meta-llama/Llama-3.2-1B-Instruct}"]

  # SGLang LLM Server - AMD ROCm profile
  sglang-rocm:
    build: ./docker/sglang-rocm
    profiles:
      - rocm
    ports:
      - "30000:30000"
    volumes:
      - ./storage/huggingface:/root/.cache/huggingface
    environment:
      - HF_TOKEN=${HF_TOKEN:-}
    networks:
      default:
        aliases:
          - sglang
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render
    command: ["python", "-m", "sglang.launch_server", "--host", "0.0.0.0", "--port", "30000", "--model-path", "${SGLANG_MODEL:-meta-llama/Llama-3.2-1B-Instruct}"]

  backend:
    build: ./backend
    ports:
      - "8010:8010"
    volumes:
      - ./storage:/app/storage
      - ./data:/app/data
      - ./backend:/app
      - ./models:/app/models
    environment:
      - COMFYUI_URL=http://comfyui:8188
      - STORAGE_PATH=/app/storage
      - DATABASE_URL=sqlite:///./data/folio.db
      - MODELS_PATH=/app/models
      - SGLANG_HOST=sglang
      - SGLANG_PORT=30000
      - DEFAULT_MODEL=${SGLANG_MODEL:-meta-llama/Llama-3.2-1B-Instruct}
      - HF_TOKEN=${HF_TOKEN:-}
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8010", "--reload"]

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://backend:8010
    depends_on:
      - backend
