name: folio

services:
  comfyui:
    build: ./docker/comfyui-cpu
    ports:
      - "8188:8188"
    volumes:
      - ./models:/opt/ComfyUI/models
      - ./storage/comfyui-output:/opt/ComfyUI/output
    environment:
      - COMFYUI_ARGS=--listen 0.0.0.0 --cpu
      - WEB_ENABLE_AUTH=false
      - SKIP_ACL=true
      - CF_QUICK_TUNNELS=false
      - SERVICEPORTAL_PORT_HOST=
      - SYNCTHING_PORT_HOST=
    profiles:
      - cpu

  comfyui-nvidia:
    build: ./docker/comfyui
    runtime: nvidia
    profiles:
      - gpu
    ports:
      - "8188:8188"
    volumes:
      - ./models:/opt/ComfyUI/models
      - ./storage/comfyui-output:/opt/ComfyUI/output
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - GPU_COUNT=1
      - COMFYUI_ARGS=--listen 0.0.0.0
      - WEB_ENABLE_AUTH=false
      - SKIP_ACL=true
      - CF_QUICK_TUNNELS=false
      - SERVICEPORTAL_PORT_HOST=
      - SYNCTHING_PORT_HOST=
    networks:
      default:
        aliases:
          - comfyui
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  comfyui-rocm:
    build: ./docker/comfyui-rocm
    profiles:
      - rocm
    ports:
      - "8188:8188"
    volumes:
      - ./models:/opt/ComfyUI/models
      - ./storage/comfyui-output:/opt/ComfyUI/output
    environment:
      - GPU_COUNT=1
      - COMFYUI_ARGS=--listen 0.0.0.0
      - WEB_ENABLE_AUTH=false
      - SKIP_ACL=true
      - CF_QUICK_TUNNELS=false
      - SERVICEPORTAL_PORT_HOST=
      - SYNCTHING_PORT_HOST=
    networks:
      default:
        aliases:
          - comfyui
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render

  # Ollama LLM Server - CPU profile
  ollama:
    image: ollama/ollama
    ports:
      - "11434:11434"
    volumes:
      - ./storage/ollama:/root/.ollama
    profiles:
      - cpu

  # Ollama LLM Server - NVIDIA GPU profile
  ollama-nvidia:
    image: ollama/ollama
    runtime: nvidia
    profiles:
      - gpu
    ports:
      - "11434:11434"
    volumes:
      - ./storage/ollama:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    networks:
      default:
        aliases:
          - ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Ollama LLM Server - AMD ROCm profile
  ollama-rocm:
    image: ollama/ollama:rocm
    profiles:
      - rocm
    ports:
      - "11434:11434"
    volumes:
      - ./storage/ollama:/root/.ollama
    networks:
      default:
        aliases:
          - ollama
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
      - render

  backend:
    build: ./backend
    ports:
      - "8010:8010"
    volumes:
      - ./storage:/app/storage
      - ./data:/app/data
      - ./backend:/app
      - ./models:/app/models
    environment:
      - COMFYUI_URL=http://comfyui:8188
      - STORAGE_PATH=/app/storage
      - DATABASE_URL=sqlite:///./data/folio.db
      - MODELS_PATH=/app/models
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
    command: ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8010", "--reload"]

  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=http://backend:8010
    depends_on:
      - backend
